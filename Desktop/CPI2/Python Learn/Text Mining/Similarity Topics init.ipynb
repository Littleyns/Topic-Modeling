{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyPDF4\n",
    "import os\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import io\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listnames = os.listdir(\"Articles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-s2.0-S0019850120301851-main.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listnames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digitize_pdf(file_path):\n",
    "    file_data = []\n",
    "    _buffer = StringIO()\n",
    "    data = parser.from_file(file_path, xmlContent=True)\n",
    "    xhtml_data = BeautifulSoup(data['content'], features=\"lxml\")\n",
    "    for page, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "        #print('Parsing page {} of pdf file...'.format(page+1))\n",
    "        _buffer.write(str(content))\n",
    "        parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        _buffer.truncate()\n",
    "        file_data.append({'id': str(page+1), 'content': parsed_content['content']})\n",
    "    return file_data\n",
    "\n",
    "def clean_document(doc):\n",
    "    for i in reversed(range(1,len(doc))) :\n",
    "        content = doc[i]['content']\n",
    "        sustract = doc[i-1]['content']\n",
    "        modified_content = content.replace(sustract, \"\")\n",
    "        doc[i]['content'] = modified_content\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(listnames)):\n",
    "    path = 'Articles/'+listnames[j]\n",
    "    tika = digitize_pdf(path)\n",
    "        #need to clean tika data \n",
    "    tika = clean_document(tika)\n",
    "    for i in range(len(tika)):\n",
    "        with io.open(\"Articlestxt/\"+listnames[j][0:-4]+\".txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(tika[i]['content'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
